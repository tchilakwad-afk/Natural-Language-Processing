# -*- coding: utf-8 -*-
"""NLP_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TuK0-VchYjYF3lyi9kKR4Hf3NT_yziI_

NLP Project 25B2208 (ADVANCED)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pandas as pd
!pip install -U transformers datasets
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import gradio as gr

file_id = '1JzAG6cIZaa2yih2kSdI-OcOkk63pGshZ'
url = f'https://drive.google.com/uc?id={file_id}'
df = pd.read_csv(url)

df['labels'] = df['stars']-1

dataset = Dataset.from_pandas(df)

dataset = dataset.train_test_split(test_size=0.4)
train_ds = dataset['train']
test_ds = dataset['test']

mono_model_name = "distilbert-base-uncased"
multi_model_name = "distilbert-base-multilingual-cased"

mono_tokenizer = AutoTokenizer.from_pretrained(mono_model_name)
multi_tokenizer = AutoTokenizer.from_pretrained(multi_model_name)

def mono_tokenize_function(examples):
  return mono_tokenizer(examples["review_body"], padding="max_length", truncation=True)

def multi_tokenize_function(examples):
  return multi_tokenizer(examples["review_body"], padding="max_length", truncation=True)

mono_train = train_ds.map(mono_tokenize_function, batched=True)
mono_test = test_ds.map(mono_tokenize_function, batched=True)

multi_train = train_ds.map(multi_tokenize_function, batched=True)
multi_test = test_ds.map(multi_tokenize_function, batched=True)

mono_model = AutoModelForSequenceClassification.from_pretrained(mono_model_name, num_labels=len(df['stars'].unique()))
multi_model = AutoModelForSequenceClassification.from_pretrained(multi_model_name, num_labels=len(df['stars'].unique()))

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=1,
    weight_decay=0.01,
    report_to="none",
    logging_strategy="epoch"
)

def compute_metrics(eval_pred):
  logits, labels = eval_pred
  predictions = np.argmax(logits, axis=-1)
  accuracy = accuracy_score(labels, predictions)
  f1 = f1_score(labels, predictions, average="weighted")
  return {"accuracy": accuracy, "f1": f1}

mono_trainer = Trainer(
    model=mono_model,
    args=training_args,
    train_dataset=mono_train,
    eval_dataset=mono_test,
    tokenizer=mono_tokenizer,
    compute_metrics=compute_metrics
)

mono_trainer.train()

multi_trainer = Trainer(
    model=multi_model,
    args=training_args,
    train_dataset=multi_train,
    eval_dataset=multi_test,
    tokenizer=multi_tokenizer,
    compute_metrics=compute_metrics
)

multi_trainer.train()

mono_metrics = mono_trainer.evaluate()
multi_metrics = multi_trainer.evaluate()

print(f"MonoLingual Metrics: {mono_metrics}")
print(f"MultiLingual Metrics: {multi_metrics}")

languages = df["language"].unique()

def filter_by_language(example, language_name):
  return example["language"] == language_name

def evaluate_per_language(trainer, test_dataset):
  results = {}
  for lang in languages:
    def filter_func(example):
      return filter_by_language(example, lang)

    lang_test = test_dataset.filter(filter_func)
    if len(lang_test) == 0:
      continue
    metrics = trainer.evaluate(lang_test)
    results[lang] = metrics
    print(f"Language: {lang}")
    print(metrics)
  return results

mono_lang_results = evaluate_per_language(mono_trainer, mono_test)
multi_lang_results = evaluate_per_language(multi_trainer, multi_test)

predictions = mono_trainer.predict(mono_test)
preds = np.argmax(mono_trainer.predict(mono_test).predictions, axis=1)
true = mono_trainer.predict(mono_test).label_ids
cm = confusion_matrix(true, preds)

sns.heatmap(cm, annot=True, fmt="d")
plt.title("Monolingual Model Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

langs = list(mono_lang_results.keys())
mono_acc = [mono_lang_results[lang]['eval_accuracy'] for lang in langs]
multi_acc = [multi_lang_results[lang]['eval_accuracy'] for lang in langs]

plt.figure(figsize=(8,5))
plt.bar(langs, mono_acc, alpha=0.7, label="Monolingual")
plt.bar(langs, multi_acc, alpha=0.7, label="Multilingual")
plt.title("Accuracy Comparision Across Languages")
plt.xlabel("Language")
plt.ylabel("Accuracy")
plt.show()

preds = np.argmax(predictions.predictions, axis=1)
df_test = test_ds.to_pandas()
df_test["pred"] = preds
df_test["correct"] = df_test["labels"] == df_test["pred"]

print("Correctly classfied examples: ")
print(df_test[df_test["correct"]].sample(3)[["review_body", "labels", "pred"]])
print("Incorrectly classified examples: ")
print(df_test[~df_test["correct"]].sample(3)[["review_body", "labels", "pred"]])

print("Observation: ")
print("Monolingual model performs better on English reviews")
print("Multilingual model generalizes better to non-English data")
print("But multiligual model takes longer to train and may have slightly lower English accuracy")

def predict_sentiment(text, model_choice="Multilingual"):
    if model_choice == "Multilingual":
        inputs = multi_tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        outputs = multi_model(**inputs)
    else:
        inputs = mono_tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        outputs = mono_model(**inputs)
    pred = torch.argmax(outputs.logits, dim=1).item()
    return f"Predicted Stars: {pred + 1}"

gr.Interface(
    fn=predict_sentiment,
    inputs=[gr.Textbox(label="Enter Review"), gr.Radio(["Monolingual", "Multilingual"])],
    outputs="text",
    title="Multilingual Review Sentiment Classifier"
).launch()