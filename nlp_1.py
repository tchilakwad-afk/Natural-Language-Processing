# -*- coding: utf-8 -*-
"""NLP_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17HOgZXgZAJVPx4Jz7IRne5mQ13YYYHOT

Assignment no. 1 answers (ADVANCED level)

Corpus for the assignment: (Source: imdb review for 'The Godfather' followed by a html file)
"""

text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""

"""Answer 1 (LowerCasing)"""

text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""
lowercased_text = text.lower()
print(lowercased_text)

"""Answer 2 (Removing punctuation and Special Characters)"""

import re
text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""
punctuation_pattern = r'[^\w\s]'
text_cleaned = re.sub(punctuation_pattern, '', text)
print(text_cleaned)

"""Answer 3 (Stop-Words Removal)"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

def remove_stopwords(text, language):
  stop_words = set(stopwords.words(language))
  word_tokens = text.split()
  filtered_text = [word for word in word_tokens if word not in stop_words]
  print(f"Language: {language}")
  print("Filtered Text: ", filtered_text)

text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""
remove_stopwords(text, "english")

"""Answer 4 (Removal of URLs)"""

def remove_urls(text):
  url_pattern = re.compile(r'https?://\S+|www\.\S+')
  return url_pattern.sub(r'',text)

text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""
remove_urls(text)

"""Answer 5 (Removal of HTML tags)"""

import re

text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""

html_tags_pattern = r'<.*?>'
text_without_html_tags = re.sub(html_tags_pattern, '', text)
print(text_without_html_tags)

"""Answer 6 (Stemming)"""

from nltk.stem.porter import PorterStemmer

stemmer = PorterStemmer()

def stem_words(text):
  word_tokens = text.split()
  stems = [stemmer.stem(word) for word in word_tokens]
  return stems

text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""
stem_words(text)

"""Answer 7 (Lemmatization)"""

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

def lemmatize_word(text):
  word_tokens = text.split()
  lemmas = [lemmatizer.lemmatize(word, pos = 'v') for word in word_tokens]
  return lemmas

text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""
print(lemmatize_word(text))

"""Answer 8 (Tokenization)"""

import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize

text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""

tokens = word_tokenize(text)
print(tokens)

"""Answer 9 (Bag of Words)"""

import nltk
nltk.download('punkt_tab')
from nltk.stem import WordNetLemmatizer
import re
from nltk.corpus import stopwords

def preprocessing_text(text):
  lemmatizer = WordNetLemmatizer()
  emoji_pattern = r'^(?:[\u2700-\u27bf]|(?:\ud83c[\udde6-\uddff]){1,2}|(?:\ud83d[\udc00-\ude4f]){1,2}|[\ud800-\udbff][\udc00-\udfff]|[\u0021-\u002f\u003a-\u0040\u005b-\u0060\u007b-\u007e]|\u3299|\u3297|\u303d|\u3030|\u24c2|\ud83c[\udd70-\udd71]|\ud83c[\udd7e-\udd7f]|\ud83c\udd8e|\ud83c[\udd91-\udd9a]|\ud83c[\udde6-\uddff]|\ud83c[\ude01-\ude02]|\ud83c\ude1a|\ud83c\ude2f|\ud83c[\ude32-\ude3a]|\ud83c[\ude50-\ude51]|\u203c|\u2049|\u25aa|\u25ab|\u25b6|\u25c0|\u25fb|\u25fc|\u25fd|\u25fe|\u2600|\u2601|\u260e|\u2611|[^\u0000-\u007F])+$'

  text = text.split()
  text = [lemmatizer.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]
  text = ' '.join(text)
  text = re.sub(r'[0-9]+', '', text)
  text = re.sub(r'[^\w\s]','', text)
  text = re.sub(emoji_pattern, '', text)
  text = re.sub(r'\s+','',text)
  text = text.lower().strip()

  return text

text = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.'

<script src="https://gist.github.com/lillylangtree/b55828fa05ed3470d352.js"></script>
<html>
<head>
  <title>Spoon-Knife</title>
</head>
<body>
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

</body>
</html>"""

sentences_list = nltk.sent_tokenize(text)

corpus = [preprocessing_text(sentence) for sentence in sentences_list]

print(corpus)

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
feature_names = vectorizer.get_feature_names_out()
X_array = X.toarray()

print("Unique Word List: \n", feature_names)
print("Bag of Words Matrix: \n", X_array)

import pandas as pd

df = pd.DataFrame(data=X_array, columns=feature_names, index=corpus)
(df)

"""Answer 10 (Term Frequency-Inverse Document Frequency)"""

from nltk.stem import WordNetLemmatizer
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
nltk.download('punkt_tab')
# Define all pre-processing steps except for Lemmatization
def preprocessing_text(text):
    emoji_pattern = r'^(?:[\u2700-\u27bf]|(?:\ud83c[\udde6-\uddff]){1,2}|(?:\ud83d[\udc00-\ude4f]){1,2}|[\ud800-\udbff][\udc00-\udfff]|[\u0021-\u002f\u003a-\u0040\u005b-\u0060\u007b-\u007e]|\u3299|\u3297|\u303d|\u3030|\u24c2|\ud83c[\udd70-\udd71]|\ud83c[\udd7e-\udd7f]|\ud83c\udd8e|\ud83c[\udd91-\udd9a]|\ud83c[\udde6-\uddff]|\ud83c[\ude01-\ude02]|\ud83c\ude1a|\ud83c\ude2f|\ud83c[\ude32-\ude3a]|\ud83c[\ude50-\ude51]|\u203c|\u2049|\u25aa|\u25ab|\u25b6|\u25c0|\u25fb|\u25fc|\u25fd|\u25fe|\u2600|\u2601|\u260e|\u2611|[^\u0000-\u007F])+$'

    text= text.lower()
    text = text.split()
    text = ' '.join(text)
    text = re.sub(r'[0-9]+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(emoji_pattern, '', text)
    text= re.sub(r'\s+', ' ', text)

    return text

comments = """A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.
Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.
Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.
Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.
If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.
This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10."""

sentences_list = nltk.sent_tokenize(comments)
corpus = [preprocessing_text(sentence) for sentence in sentences_list]
corpus

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)
terms = tfidf_vectorizer.get_feature_names_out()
df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)

df

texts = [
    "A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.",
    "Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.",
    "Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.",
    "Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.",
    "If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.",
    "This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.",
]

import re

def clean_text(text):
    clean_txt = text.lower()
    # Clear characters other than numbers and letters of the alphabet only
    clean_txt = re.sub(r'[^0-9a-zçğıiöşü\s]', '', clean_txt,
                       flags=re.IGNORECASE)

    return ' '.join(sorted(clean_txt.split()))

cleaned_texts = [clean_text(text) for text in texts]

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(cleaned_texts)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(vectorizer.vocabulary_)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""Answer 11 (Encodings)"""

import nltk
from sklearn.preprocessing import OneHotEncoder
import numpy as np

nltk.download('punkt')

corpus = [
    "A masterclass in film making, is The Godfather a contender for the best film of all time? Id argue the case that it is, this is the ultimate gangster movie.",
    "Before you panic at the thought of a film being almost three hours long, you neednt, you wont even notice the time, it flies by.",
    "Production values are incredible, it looks sublime the whole way through, it's so well produced, at roughly fifty years old it puts many new films to shame.",
    "Brandon, Pacino and Castellano, just a few of the Incredible performances, I could add a whole lot more.",
    "If you're considering buying a hard copy, I would recommend it on blu ray, it is sharper than the dvd, there is a difference.",
    "This film has had a huge influence down the years, it is still, and will forever be, one of the greatest, 10/10.",
]

tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]

all_words = [word for sentence in tokenized_corpus for word in sentence]

vocab = sorted(set(all_words))

print("Vocalbulary: ", vocab)

word_array = np.array(all_words).reshape(-1,1)

one_hot_encoder = OneHotEncoder(sparse_output=False)
one_hot_encoded = one_hot_encoder.fit_transform(word_array)

print("One-hot encoded matrix:\n", one_hot_encoded)

"""Answer 12 (Word2Vec)"""

!pip install gensim
import gensim
from gensim.models import Word2Vec

sentences = [
    ['A', 'masterclass', 'in', 'film', 'making', 'is', 'The', 'Godfather', 'a', 'contender', 'for', 'the', 'best', 'film', 'of', 'all', 'time?', 'Id', 'argue', 'the', 'case', 'that', 'it', 'is', 'this', 'is', 'the', 'ultimate', 'gangster', 'movie.'],
    ['Before', 'you', 'panic', 'at', 'the', 'thought', 'of', 'a', 'film', 'being', 'almost', 'three', 'hours', 'long', 'you', 'neednt', 'you', 'wont', 'even', 'notice', 'the', 'time', 'it', 'flies', 'by.'],
    ['Production', 'values', 'are', 'incredible', 'it', 'looks', 'sublime', 'the', 'whole', 'way', 'through', 'its', 'so', 'well', 'produced', 'at', 'roughly', 'fifty', 'years', 'old', 'it', 'puts', 'many', 'new', 'films', 'to', 'shame.'],
    ['Brandon', 'Pacino', 'and', 'Castellano', 'just', 'a', 'few', 'of', 'the', 'Incredible', 'performances', 'I', 'could', 'add', 'a', 'whole', 'lot', 'more.'],
    ['If', 'youre', 'considering', 'buying', 'a', 'hard', 'copy', 'I', 'would', 'recommend', 'it', 'on', 'blu', 'ray', 'it', 'is', 'sharper', 'than', 'the', 'dvd', 'there', 'is', 'a', 'difference.'],
    ['This', 'film', 'has', 'had', 'a', 'huge', 'influence', 'down', 'the', 'years', 'it', 'is', 'still', 'and', 'will', 'forever', 'be', 'one', 'of', 'the', 'greatest', '10/10.']
]

cbow_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0, alpha=0.03, min_alpha=0.0007, epochs=100)
skipgram_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1, alpha=0.03, min_alpha=0.0007, epochs=100)

cbow_model.train(sentences, total_examples=len(sentences), epochs=100)
skipgram_model.train(sentences, total_examples=len(sentences), epochs=100)

word_vectors_cbow = cbow_model.wv
similarity_cbow = word_vectors_cbow.similarity('film', 'movie.')
print(f"Similarity between 'film' and 'movie.': {similarity_cbow} with CBOW")


word_vectors_skipgram= skipgram_model.wv
similarity_skip = word_vectors_skipgram.similarity('film', 'movie.')
print(f"Similarity between 'film' and 'movie.': {similarity_skip} with Skip-Gram")