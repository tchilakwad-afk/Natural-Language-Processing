# -*- coding: utf-8 -*-
"""NLP_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z6mL8crVFYIOgdpDaAQwc9r5Cbc1VRaB

Answers to assignment 2: (ADVANCED level)
"""

import pandas as pd
import matplotlib.pyplot as plt
import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
!pip install gensim
import gensim
from gensim import corpora
from sklearn.metrics import classification_report

file_id = '1a8UVk2Z1dh1TF7-m0XYj_1-atj_2TZFt'
url = f'https://drive.google.com/uc?id={file_id}'
df = pd.read_csv(url)
df

def map_sentiment(stars_received):
  if stars_received <= 2:
    return -1
  elif stars_received == 3:
    return 0
  else:
    return 1

df['sentiment'] = [map_sentiment(x) for x in df['Labels']]

plt.figure()
pd.value_counts(df['sentiment']).plot.bar(title="Sentiment distribution in df")
plt.xlabel("Sentiment")
plt.ylabel("Number of rows in df")
plt.show()

df['tokenized_text'] = [word_tokenize(x) for x in (df['Text'])]
print(df['tokenized_text'].head(10))

ps = PorterStemmer()
df['stemmed_tokens'] = [[ps.stem(word) for word in tokens] for tokens in (df['tokenized_text'])]
print(df['stemmed_tokens'].head(10))

def split_train_test(df, test_size = 0.3, shuffle_state = True):
  X_train, X_test, Y_train, Y_test = train_test_split(df[["Text", "Labels", "tokenized_text", "stemmed_tokens"]], df['sentiment'], shuffle=shuffle_state, test_size=test_size, random_state=15)
  print("Value counts for Train sentiments")
  print(Y_train.value_counts())
  print("Value counts for Test sentiments")
  print(Y_test.value_counts())
  print(type(X_train))
  print(type(Y_train))
  X_train = X_train.reset_index()
  X_test = X_test.reset_index()
  Y_train = Y_train.to_frame()
  Y_train = Y_train.reset_index()
  Y_test = Y_test.to_frame()
  Y_test = Y_test.reset_index()
  print(X_train.head())
  return X_train, X_test, Y_train, Y_test

X_train, X_test, Y_train, Y_test = split_train_test(df)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device available for running: ")
print(device)

class FeedforwardNeuralNetModel(nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim):
    super(FeedforwardNeuralNetModel, self).__init__()
    self.fc1 = nn.Linear(input_dim, hidden_dim)
    self.relu1 = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, hidden_dim)
    self.relu2 = nn.ReLU()
    self.fc3 = nn.Linear(hidden_dim, output_dim)

  def forward(self, x):
    out = self.fc1(x)
    out = self.relu1(out)
    out = self.fc2(out)
    out = self.relu2(out)
    out = self.fc3(out)
    return F.softmax(out, dim=1)

def make_dict(top_data_df_small, padding=True):
    if padding:
        print("Dictionary with padded token added")
        review_dict = corpora.Dictionary([['pad']])
        review_dict.add_documents(top_data_df_small['stemmed_tokens'])
    else:
        print("Dictionary without padding")
        review_dict = corpora.Dictionary(top_data_df_small['stemmed_tokens'])
    return review_dict

review_dict = make_dict(df, padding=False)

VOCAB_SIZE = 30056
NUM_LABELS = 3

def make_bow_vector(review_dict, sentence):
  vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64, device=device)
  for word in sentence:
    vec[review_dict.token2id[word]]+=1
  return vec.view(1,-1).float()

def make_target(label):
  if label== -1:
    return torch.tensor([0], dtype=torch.long, device=device)
  elif label== 0:
    return torch.tensor([1], dtype=torch.long, device=device)
  else:
    return torch.tensor([2], dtype=torch.long, device=device)

VOCAB_SIZE = len(review_dict)

input_dim = VOCAB_SIZE
hidden_dim = 500
output_dim = 3
num_epochs = 100

ff_nn_bow_model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)
ff_nn_bow_model = ff_nn_bow_model.to(device)
loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(ff_nn_bow_model.parameters(), lr=0.001)

ffnn_loss_file_name = 'ffnn_bow_class_big_loss_500_epoch_100_less_lr.csv'
f = open(ffnn_loss_file_name,'w')
f.write('iter, loss')
f.write('\n')
losses = []
iter = 0

for epoch in range(num_epochs):
  if (epoch+1) % 25 == 0:
    print("Epoch completed: " + str(epoch+1))
  train_loss = 0
  for index, row in X_train.iterrows():
    optimizer.zero_grad()
    bow_vec = make_bow_vector(review_dict, row['stemmed_tokens'])
    probs = ff_nn_bow_model(bow_vec)
    target = make_target(Y_train['sentiment'][index])
    loss = loss_function(probs, target)
    train_loss += loss.item()
    loss.backward()
    optimizer.step()
  f.write(str((epoch+1)) + "," + str(train_loss / len(X_train)))
  f.write('\n')
  train_loss = 0

f.close()

bow_ff_nn_predictions = []
original_lables_ff_bow = []

with torch.no_grad():
    for index, row in X_test.iterrows():
        bow_vec = make_bow_vector(review_dict, row['stemmed_tokens'])
        probs = ff_nn_bow_model(bow_vec)
        bow_ff_nn_predictions.append(torch.argmax(probs, dim=1).cpu().numpy()[0])
        original_lables_ff_bow.append(make_target(Y_test['sentiment'][index]).cpu().numpy()[0])

print(classification_report(original_lables_ff_bow,bow_ff_nn_predictions))
ffnn_loss_df = pd.read_csv(ffnn_loss_file_name)
print(len(ffnn_loss_df))
print(ffnn_loss_df.columns)
ffnn_plt_500_padding_100_epochs = ffnn_loss_df[' loss'].plot()
fig = ffnn_plt_500_padding_100_epochs.get_figure()
fig.savefig("ffnn_bow_loss_500_padding_100_epochs_less_lr.jpg")


#ADVANCED


class FeedforwardNeuralNetModel_dropout(nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):
    super(FeedforwardNeuralNetModel_dropout, self).__init__()
    self.fc1 = nn.Linear(input_dim, hidden_dim)
    self.relu1 = nn.ReLU()
    self.dropout1 = nn.Dropout(p=dropout_rate)
    self.fc2 = nn.Linear(hidden_dim, hidden_dim)
    self.relu2 = nn.ReLU()
    self.dropout2 = nn.Dropout(p=dropout_rate)
    self.fc3 = nn.Linear(hidden_dim, output_dim)

  def forward(self, x):
    out = self.fc1(x)
    out = self.relu1(out)
    out = self.dropout1(out)
    out = self.fc2(out)
    out = self.relu2(out)
    out = self.dropout2(out)
    out = self.fc3(out)
    return F.softmax(out, dim=1)

dropout_rate = 0.3

ff_nn_bow_model_dropout = FeedforwardNeuralNetModel_dropout(input_dim, hidden_dim, output_dim, dropout_rate)
ff_nn_bow_model_dropout = ff_nn_bow_model_dropout.to(device)

loss_function_dropout = nn.CrossEntropyLoss()
optimizer_dropout = torch.optim.SGD(ff_nn_bow_model_dropout.parameters(), lr=0.001)

ffnn_loss_file_name_dropout = 'ffnn_bow_class_big_loss_500_epoch_100_less_lr_dropout.csv'
f_dropout = open(ffnn_loss_file_name_dropout, 'w')
f_dropout.write('iter, loss\n')
losses_dropout = []
iter_dropout = 0

ff_nn_bow_model_dropout.train()
for epoch in range(num_epochs):
  if (epoch + 1) % 25 == 0:
    print(f"Epoch completed: {epoch + 1}")

  train_loss_dropout = 0

  for index, row in X_train.iterrows():
    optimizer_dropout.zero_grad()
    bow_vec = make_bow_vector(review_dict, row['stemmed_tokens'])
    probs = ff_nn_bow_model_dropout(bow_vec)
    target = make_target(Y_train['sentiment'][index])
    loss = loss_function_dropout(probs, target)
    train_loss_dropout += loss.item()
    loss.backward()
    optimizer_dropout.step()

  f_dropout.write(f"{epoch + 1},{train_loss_dropout / len(X_train)}\n")
  train_loss_dropout = 0

f_dropout.close()

bow_ff_nn_predictions_dropout = []
original_lables_ff_bow_dropout = []

ff_nn_bow_model_dropout.eval()
with torch.no_grad():
    for index, row in X_test.iterrows():
        bow_vec = make_bow_vector(review_dict, row['stemmed_tokens'])
        probs = ff_nn_bow_model_dropout(bow_vec)
        bow_ff_nn_predictions_dropout.append(torch.argmax(probs, dim=1).cpu().numpy()[0])
        original_lables_ff_bow_dropout.append(make_target(Y_test['sentiment'][index]).cpu().numpy()[0])

print("Classification Report with Dropout:")
print(classification_report(original_lables_ff_bow_dropout, bow_ff_nn_predictions_dropout))

ffnn_loss_df = pd.read_csv(ffnn_loss_file_name)
ffnn_loss_df_dropout = pd.read_csv(ffnn_loss_file_name_dropout)

plt.figure(figsize=(10, 6))
plt.plot(ffnn_loss_df['iter'], ffnn_loss_df[' loss'], label='Without Dropout')
plt.plot(ffnn_loss_df_dropout['iter'], ffnn_loss_df_dropout[' loss'], label='With Dropout')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss with and Without Dropout')
plt.legend()
plt.grid(True)
plt.savefig("ffnn_bow_loss_comparison_dropout.jpg")
plt.show()